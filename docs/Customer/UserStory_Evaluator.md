# User Story: LLM Evaluator

**As a** security researcher and AI enthusiast,
**I want to** participate in competitive "wargames" and "tournaments" to test the limits of various large language models,
**so that I can** identify their weaknesses, biases, and vulnerabilities, contribute to making AI safer, and potentially earn rewards and recognition for my findings.

## Acceptance Criteria

* **Access to Models:** I can access a variety of LLMs through a unified interface.
* **Competitive Scenarios:** I can participate in structured "wargames" and "tournaments" with specific goals and rules.
* **Red Teaming:** I can engage in "red teaming" exercises to try and elicit harmful, biased, or otherwise undesirable responses from the models.
* **Anonymized Testing:** The models I test are anonymized to ensure my evaluation is unbiased.
* **Leaderboards and Rankings:** I can see my ranking and progress on leaderboards for different challenges and tournaments.
* **Submission of Findings:** I can submit my findings (e.g., successful "jailbreaks," examples of bias) for evaluation.
* **Rewards and Recognition:** I can earn badges and other rewards for my performance and contributions.
* **Secure Authentication:** I can securely log in to the platform using a passwordless method (email OTP) to track my progress and rewards.
